{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris Dong\\.conda\\envs\\semantic_similarity\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas() # pretty helpful to determine time needed for pandas to run shit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just a quick warning - this notebook is gonna take some space in your computer btw. sorry in advance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris Dong\\.conda\\envs\\semantic_similarity\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Found cached dataset stsb_multi_mt (C:/Users/Chris Dong/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9)\n",
      "100%|██████████| 3/3 [00:00<00:00, 500.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749, 3) (1379, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence1  \\\n",
       "0                    A girl is styling her hair.   \n",
       "1       A group of men play soccer on the beach.   \n",
       "2  One woman is measuring another woman's ankle.   \n",
       "3                A man is cutting up a cucumber.   \n",
       "4                       A man is playing a harp.   \n",
       "\n",
       "                                          sentence2  similarity_score  \n",
       "0                      A girl is brushing her hair.               2.5  \n",
       "1  A group of boys are playing soccer on the beach.               3.6  \n",
       "2           A woman measures another woman's ankle.               5.0  \n",
       "3                      A man is slicing a cucumber.               4.2  \n",
       "4                      A man is playing a keyboard.               1.5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stsb_dataset = load_dataset('stsb_multi_mt', 'en')\n",
    "stsb_train = pd.DataFrame(stsb_dataset['train'])\n",
    "stsb_test = pd.DataFrame(stsb_dataset['test'])\n",
    "\n",
    "# Check loaded data\n",
    "print(stsb_train.shape, stsb_test.shape)\n",
    "stsb_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Similarity\n",
    "\n",
    "You gotta remove stopwords, lowercase and lemmatize before running the algo so it uses only informative words in the calc\n",
    "\n",
    "Jaccard uses 1 gram, if you want N-grams then it would be w-shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1379/1379 [00:16<00:00, 84.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import textdistance\n",
    "from helper import text_processing\n",
    "\n",
    "def jaccard_sim(row):\n",
    "    # Text Processing\n",
    "    sentence1 = text_processing(row['sentence1'])\n",
    "    sentence2 = text_processing(row['sentence2'])\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    return textdistance.jaccard.normalized_similarity(sentence1, sentence2)\n",
    "\n",
    "\n",
    "# Jaccard Similarity\n",
    "stsb_test['Jaccard_score'] = stsb_test.progress_apply(jaccard_sim, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words - \n",
    "\n",
    "Standard TFIDF vectorizer and count vectorizer to compare embedding vectors by computing the cosine similarities\n",
    "\n",
    "pros and cons: Count vector treats all words equally important - ew. \n",
    "\n",
    "TFDIF utilize Term Frequency (TF) and Inverse Document Frequency (IDF) - \n",
    "\n",
    "TF - how many times the word appears in the doc, meausres how important the word is to the doc \n",
    "IDF - log inverse of the fraction of the document in which the word appears. Measures how rare the word is in the corpus\n",
    "\n",
    "Normalizing the dataset needs to happen so the document length doesnt skew the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from helper import cos_sim\n",
    "\n",
    "model = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Train the model\n",
    "X_train = pd.concat([stsb_train['sentence1'], stsb_train['sentence2']]).unique()\n",
    "model.fit(X_train)\n",
    "\n",
    "# Generate Embeddings on Test\n",
    "sentence1_emb = model.transform(stsb_test['sentence1'])\n",
    "sentence2_emb = model.transform(stsb_test['sentence2'])\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['TFIDF_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Movers Distance (WMD)\n",
    " Jaccard and TFIDF assumes that similar texts have many words in common - however given the statement:\n",
    "\n",
    " Obama speaks to the media in Illinois \n",
    " The president greets the press in Chicago \n",
    "\n",
    "The use of word embeddings are needed to demonstrate similar words have vectors near each other in vector space -\n",
    "president - obama, Chicago - Illinois, greets - speaks, media - press\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1379/1379 [00:15<00:00, 90.08it/s] \n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "def word_movers_distance(row):\n",
    "    # Text Processing\n",
    "    sentence1 = text_processing(row['sentence1'])\n",
    "    sentence2 = text_processing(row['sentence2'])\n",
    "    \n",
    "    # Negative Word Movers Distance\n",
    "    return -model.wmdistance(sentence1, sentence2)\n",
    "\n",
    "\n",
    "# Negative Word Movers Distance\n",
    "stsb_test['NegWMD_score'] = stsb_test.progress_apply(word_movers_distance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations of WMD is that the word embeddings used in WMD are non-contextual, where each word gets the same embedding vector irrespective of the context of the rest of the sentence in which it appears.\n",
    "Future nlp algos are designed to handle this problem with transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Sentence Encoder (USE)\n",
    "\n",
    "pretrained transformer model on multi-task obj, then used it for transfer learning. \n",
    "\n",
    "1) compute the contextual word embedding for each word, then \n",
    "2) compute the sentence embedding by performing element wise sum of all word vectors and dividing by the square root of the length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load the pre-trained model\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    # Control GPU memory usage\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n",
    "model = hub.load(module_url)\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model(stsb_test['sentence1']).numpy()\n",
    "sentence2_emb = model(stsb_test['sentence2']).numpy()\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['USE_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contextual sentence embeddings works by transformers in a transfer learning setting. Metric learning is used to get better performance on subsequent models\n",
    "\n",
    "# Cross encoder\n",
    "\n",
    "BERT - Bi direction encoder rep (2018) - brought some neat models - DistilBert, AlBert, RoBerta\n",
    "\n",
    "Self-supervised pre-training callled masked lang model. Hide some words and train the model to predict the missing words given the words before and after (ie bi directional). This allowed bert to understand the semantic relationship between words. \n",
    "\n",
    "We can use Bert as a cross encoder - adding a classification head to the output of the bert model. We can use the encoder take in a pair of text and output the probability that two texts are similar\n",
    "\n",
    "Note: Cross-encoders do not output any embedding vectors and are thus not very scalable beyond a few thousands of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 608/608 [00:00<00:00, 607kB/s]\n",
      "c:\\Users\\Chris Dong\\.conda\\envs\\semantic_similarity\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Chris Dong\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 499M/499M [00:33<00:00, 15.1MB/s] \n",
      "Downloading: 100%|██████████| 142/142 [00:00<00:00, 142kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 7.11MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 5.50MB/s]\n",
      "Downloading: 100%|██████████| 772/772 [00:00<00:00, 380kB/s]\n",
      "Batches: 100%|██████████| 44/44 [01:00<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = CrossEncoder('cross-encoder/stsb-roberta-base')\n",
    "\n",
    "sentence_pairs = []\n",
    "for sentence1, sentence2 in zip(stsb_test['sentence1'], stsb_test['sentence2']):\n",
    "    sentence_pairs.append([sentence1, sentence2])\n",
    "    \n",
    "stsb_test['SBERT CrossEncoder_score'] = model.predict(sentence_pairs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Learning\n",
    "\n",
    "Notes: Promising ways to generate embeddings, especially for similarity search applications\n",
    "\n",
    "1) Use a Neural network such as a BERT to convert texts to embeddings\n",
    "2) Construct these embeddings so that semantically similar texts cluster nearer to each other while dissimilar texts are further apart.\n",
    "\n",
    "After training a model with this appraoch you can find similarities between two text by computing the cosine similarity between the two vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SBERT Bi-Encoder\n",
    "\n",
    "Using BERT and its variants as the base model and is pre-trained utilizing a type of metric learning called contrastive learning. In contrastive learning, the contrastive loss function compares whether two embeddings are similar (0) or dissimilar (1).\n",
    "\n",
    "Core ideas\n",
    "\n",
    "1) Using the labeled SNLI dataset and STS. These datasets contain several thousand pairs of sentences labeled as either similar or dissimilar.\n",
    "\n",
    "2) For each text in the training dataset, compute the contextual word embeddings of that text using any pre-trained BERT model as an encoder.\n",
    "\n",
    "3) compute the element-wise average of all token embeddings to obtain a single fixed dimension sentence embedding for the entire text - this is called mean pooling\n",
    "\n",
    "4) train the model using the Siamese network arch. Essential both pair of text is subjected to Bert -> Mean pooling -> embedding -> Cosine similarity \n",
    "\n",
    "5) Finally do a cosine similarity\n",
    "\n",
    "Bi-encoders are great at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 868/868 [00:00<00:00, 868kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 196kB/s]\n",
      "Downloading: 100%|██████████| 3.67k/3.67k [00:00<00:00, 3.67MB/s]\n",
      "Downloading: 100%|██████████| 588/588 [00:00<00:00, 572kB/s]\n",
      "Downloading: 100%|██████████| 122/122 [00:00<00:00, 61.0kB/s]\n",
      "Downloading: 100%|██████████| 438M/438M [00:28<00:00, 15.2MB/s] \n",
      "Downloading: 100%|██████████| 52.0/52.0 [00:00<00:00, 25.9kB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 120kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 4.66MB/s]\n",
      "Downloading: 100%|██████████| 1.19k/1.19k [00:00<00:00, 1.19MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 4.49MB/s]\n",
      "Downloading: 100%|██████████| 229/229 [00:00<00:00, 236kB/s]\n",
      "Batches: 100%|██████████| 44/44 [00:25<00:00,  1.70it/s]\n",
      "Batches: 100%|██████████| 44/44 [00:25<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('stsb-mpnet-base-v2')\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model.encode(stsb_test['sentence1'], show_progress_bar=True)\n",
    "sentence2_emb = model.encode(stsb_test['sentence2'], show_progress_bar=True)\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['SBERT BiEncoder_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welp the problem is that SBERT Bi-Encoder requires a fully labeled corpus of sentence pairs to train. \n",
    "\n",
    "So legit going into another field is going to be hella time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimCSE Simple Contrastive Learning of Sentence Embeddings\n",
    "\n",
    "Works in both supervised and unsupervised \n",
    "\n",
    "Core idea:\n",
    "\n",
    "1) Given a text - compute the embeddings of the text using pre-train bert as an encoder and take the embeddings of the CLS token \n",
    "\n",
    "2) create two noisy versions of the same text embedding by applting two different dropout masks on the original embedding. Two noisy embeddings generated from the same input text are considered a positive pair. Model is expected to have a cosine distance of 0\n",
    "\n",
    "3) We consider the embeddings from all the other texts in the batch as “negatives.” The model expects the “negatives” to have a cosine distance of 1 to the target text embeddings from the previous step. The loss function then updates the parameters of the encoder model such that the embeddings move closer to our expectations.\n",
    "\n",
    "4) Supervised SimCSE has one additional step where we use a Natural Lang Inference labeled data to obtain positive pairs from texts labeled entailment and negative pairs from texts are labeled contradiction. \n",
    "\n",
    "SimCSE models are Bi-Encoder Sentence Transformer using SimCSE approach. So we can reuse all the code from the bi-encoder sentence but change the pretrain model to sim CSE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 736/736 [00:00<00:00, 718kB/s]\n",
      "Downloading: 100%|██████████| 664/664 [00:00<00:00, 327kB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 4.60MB/s]\n",
      "Downloading: 100%|██████████| 1.42G/1.42G [01:33<00:00, 15.2MB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 238kB/s]\n",
      "Downloading: 100%|██████████| 256/256 [00:00<00:00, 256kB/s]\n",
      "Downloading: 100%|██████████| 798k/798k [00:00<00:00, 7.48MB/s]\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name C:\\Users\\Chris Dong/.cache\\torch\\sentence_transformers\\princeton-nlp_sup-simcse-roberta-large. Creating a new one with MEAN pooling.\n",
      "Batches: 100%|██████████| 44/44 [01:24<00:00,  1.92s/it]\n",
      "Batches: 100%|██████████| 44/44 [01:26<00:00,  1.98s/it]\n",
      "Downloading: 100%|██████████| 736/736 [00:00<00:00, 714kB/s]\n",
      "Downloading: 100%|██████████| 743/743 [00:00<00:00, 367kB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 4.88MB/s]\n",
      "Downloading: 100%|██████████| 1.42G/1.42G [01:35<00:00, 15.0MB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 120kB/s]\n",
      "Downloading: 100%|██████████| 256/256 [00:00<00:00, 85.4kB/s]\n",
      "Downloading: 100%|██████████| 798k/798k [00:00<00:00, 6.09MB/s]\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name C:\\Users\\Chris Dong/.cache\\torch\\sentence_transformers\\princeton-nlp_unsup-simcse-roberta-large. Creating a new one with MEAN pooling.\n",
      "Batches: 100%|██████████| 44/44 [01:28<00:00,  2.01s/it]\n",
      "Batches: 100%|██████████| 44/44 [01:25<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "########## Supervised ##########\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('princeton-nlp/sup-simcse-roberta-large')\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model.encode(stsb_test['sentence1'], show_progress_bar=True)\n",
    "sentence2_emb = model.encode(stsb_test['sentence2'], show_progress_bar=True)\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['SimCSE Supervised_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)\n",
    "\n",
    "\n",
    "########## Un-Supervised ##########\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('princeton-nlp/unsup-simcse-roberta-large')\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model.encode(stsb_test['sentence1'], show_progress_bar=True)\n",
    "sentence2_emb = model.encode(stsb_test['sentence2'], show_progress_bar=True)\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['SimCSE Unsupervised_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jaccard_score</th>\n",
       "      <td>66.026529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFIDF_cosine_score</th>\n",
       "      <td>61.420989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NegWMD_score</th>\n",
       "      <td>67.032848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USE_cosine_score</th>\n",
       "      <td>77.085989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBERT CrossEncoder_score</th>\n",
       "      <td>90.172534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBERT BiEncoder_cosine_score</th>\n",
       "      <td>88.572413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimCSE Supervised_cosine_score</th>\n",
       "      <td>87.082275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimCSE Unsupervised_cosine_score</th>\n",
       "      <td>82.784251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  similarity_score\n",
       "Jaccard_score                            66.026529\n",
       "TFIDF_cosine_score                       61.420989\n",
       "NegWMD_score                             67.032848\n",
       "USE_cosine_score                         77.085989\n",
       "SBERT CrossEncoder_score                 90.172534\n",
       "SBERT BiEncoder_cosine_score             88.572413\n",
       "SimCSE Supervised_cosine_score           87.082275\n",
       "SimCSE Unsupervised_cosine_score         82.784251"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_cols = [col for col in stsb_test.columns if '_score' in col]\n",
    "\n",
    "# Spearman Rank Correlation\n",
    "spearman_rank_corr = stsb_test[score_cols].corr(method='spearman').iloc[1:, 0:1]*100\n",
    "spearman_rank_corr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Chris Dong\\Desktop\\codeBase\\nlp-starter\\nlp-starter-notebook.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chris%20Dong/Desktop/codeBase/nlp-starter/nlp-starter-notebook.ipynb#ch0000026?line=13'>14</a>\u001b[0m     fig\u001b[39m.\u001b[39madd_trace(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chris%20Dong/Desktop/codeBase/nlp-starter/nlp-starter-notebook.ipynb#ch0000026?line=14'>15</a>\u001b[0m         go\u001b[39m.\u001b[39mScatter(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chris%20Dong/Desktop/codeBase/nlp-starter/nlp-starter-notebook.ipynb#ch0000026?line=15'>16</a>\u001b[0m             x\u001b[39m=\u001b[39mstsb_test[score_cols[\u001b[39m0\u001b[39m]], \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chris%20Dong/Desktop/codeBase/nlp-starter/nlp-starter-notebook.ipynb#ch0000026?line=19'>20</a>\u001b[0m         row\u001b[39m=\u001b[39mrow\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, col\u001b[39m=\u001b[39mcol\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chris%20Dong/Desktop/codeBase/nlp-starter/nlp-starter-notebook.ipynb#ch0000026?line=20'>21</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chris%20Dong/Desktop/codeBase/nlp-starter/nlp-starter-notebook.ipynb#ch0000026?line=23'>24</a>\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(height\u001b[39m=\u001b[39m\u001b[39m700\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, title_text\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSpearman Rank Correlation (ρ × 100)\u001b[39m\u001b[39m'\u001b[39m, showlegend\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Chris%20Dong/Desktop/codeBase/nlp-starter/nlp-starter-notebook.ipynb#ch0000026?line=24'>25</a>\u001b[0m fig\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\Chris Dong\\.conda\\envs\\semantic_similarity\\lib\\site-packages\\plotly\\basedatatypes.py:3398\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3365\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3366\u001b[0m \u001b[39mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[0;32m   3367\u001b[0m \u001b[39mspecified by the renderer argument\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3394\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3395\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3396\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[1;32m-> 3398\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Chris Dong\\.conda\\envs\\semantic_similarity\\lib\\site-packages\\plotly\\io\\_renderers.py:396\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    393\u001b[0m         )\n\u001b[0;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m LooseVersion(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m LooseVersion(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    397\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    398\u001b[0m         )\n\u001b[0;32m    400\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    402\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "nrows = 4\n",
    "ncols = 3\n",
    "plot_array = np.arange(0, nrows*ncols).reshape(nrows, ncols)\n",
    "\n",
    "subplot_titles = [f'{row.Index.split(\"_\")[0]}: {row.similarity_score:.2f}' for row in spearman_rank_corr.itertuples()]\n",
    "fig = make_subplots(rows=nrows, cols=ncols, subplot_titles=subplot_titles)\n",
    "\n",
    "for index, score in enumerate(spearman_rank_corr.index):\n",
    "    row, col = np.argwhere(plot_array == index)[0]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=stsb_test[score_cols[0]], \n",
    "            y=stsb_test[score],\n",
    "            mode='markers',\n",
    "        ),\n",
    "        row=row+1, col=col+1\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(height=700, width=1000, title_text='Spearman Rank Correlation (ρ × 100)', showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('semantic_similarity')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f48c2a4e05cf5de530ce7aec5f426fdf131a1126e41e99b5a2de656128f38b0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
