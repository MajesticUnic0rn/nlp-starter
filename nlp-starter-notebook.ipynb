{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris Dong\\.conda\\envs\\semantic_similarity\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas() # pretty helpful to determine time needed for pandas to run shit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just a quick warning - this notebook is gonna take some space in your computer btw. sorry in advance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris Dong\\.conda\\envs\\semantic_similarity\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Found cached dataset stsb_multi_mt (C:/Users/Chris Dong/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9)\n",
      "100%|██████████| 3/3 [00:00<00:00, 500.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749, 3) (1379, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence1  \\\n",
       "0                    A girl is styling her hair.   \n",
       "1       A group of men play soccer on the beach.   \n",
       "2  One woman is measuring another woman's ankle.   \n",
       "3                A man is cutting up a cucumber.   \n",
       "4                       A man is playing a harp.   \n",
       "\n",
       "                                          sentence2  similarity_score  \n",
       "0                      A girl is brushing her hair.               2.5  \n",
       "1  A group of boys are playing soccer on the beach.               3.6  \n",
       "2           A woman measures another woman's ankle.               5.0  \n",
       "3                      A man is slicing a cucumber.               4.2  \n",
       "4                      A man is playing a keyboard.               1.5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stsb_dataset = load_dataset('stsb_multi_mt', 'en')\n",
    "stsb_train = pd.DataFrame(stsb_dataset['train'])\n",
    "stsb_test = pd.DataFrame(stsb_dataset['test'])\n",
    "\n",
    "# Check loaded data\n",
    "print(stsb_train.shape, stsb_test.shape)\n",
    "stsb_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Similarity\n",
    "\n",
    "You gotta remove stopwords, lowercase and lemmatize before running the algo so it uses only informative words in the calc\n",
    "\n",
    "Jaccard uses 1 gram, if you want N-grams then it would be w-shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1379/1379 [00:16<00:00, 84.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import textdistance\n",
    "from helper import text_processing\n",
    "\n",
    "def jaccard_sim(row):\n",
    "    # Text Processing\n",
    "    sentence1 = text_processing(row['sentence1'])\n",
    "    sentence2 = text_processing(row['sentence2'])\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    return textdistance.jaccard.normalized_similarity(sentence1, sentence2)\n",
    "\n",
    "\n",
    "# Jaccard Similarity\n",
    "stsb_test['Jaccard_score'] = stsb_test.progress_apply(jaccard_sim, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words - \n",
    "\n",
    "Standard TFIDF vectorizer and count vectorizer to compare embedding vectors by computing the cosine similarities\n",
    "\n",
    "pros and cons: Count vector treats all words equally important - ew. \n",
    "\n",
    "TFDIF utilize Term Frequency (TF) and Inverse Document Frequency (IDF) - \n",
    "\n",
    "TF - how many times the word appears in the doc, meausres how important the word is to the doc \n",
    "IDF - log inverse of the fraction of the document in which the word appears. Measures how rare the word is in the corpus\n",
    "\n",
    "Normalizing the dataset needs to happen so the document length doesnt skew the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from helper import cos_sim\n",
    "\n",
    "model = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Train the model\n",
    "X_train = pd.concat([stsb_train['sentence1'], stsb_train['sentence2']]).unique()\n",
    "model.fit(X_train)\n",
    "\n",
    "# Generate Embeddings on Test\n",
    "sentence1_emb = model.transform(stsb_test['sentence1'])\n",
    "sentence2_emb = model.transform(stsb_test['sentence2'])\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['TFIDF_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Movers Distance (WMD)\n",
    " Jaccard and TFIDF assumes that similar texts have many words in common - however given the statement:\n",
    "\n",
    " Obama speaks to the media in Illinois \n",
    " The president greets the press in Chicago \n",
    "\n",
    "The use of word embeddings are needed to demonstrate similar words have vectors near each other in vector space -\n",
    "president - obama, Chicago - Illinois, greets - speaks, media - press\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "def word_movers_distance(row):\n",
    "    # Text Processing\n",
    "    sentence1 = text_processing(row['sentence1'])\n",
    "    sentence2 = text_processing(row['sentence2'])\n",
    "    \n",
    "    # Negative Word Movers Distance\n",
    "    return -model.wmdistance(sentence1, sentence2)\n",
    "\n",
    "\n",
    "# Negative Word Movers Distance\n",
    "stsb_test['NegWMD_score'] = stsb_test.progress_apply(word_movers_distance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations of WMD is that the word embeddings used in WMD are non-contextual, where each word gets the same embedding vector irrespective of the context of the rest of the sentence in which it appears.\n",
    "Future nlp algos are designed to handle this problem with transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load the pre-trained model\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    # Control GPU memory usage\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n",
    "model = hub.load(module_url)\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model(stsb_test['sentence1']).numpy()\n",
    "sentence2_emb = model(stsb_test['sentence2']).numpy()\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['USE_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('semantic_similarity')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f48c2a4e05cf5de530ce7aec5f426fdf131a1126e41e99b5a2de656128f38b0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
